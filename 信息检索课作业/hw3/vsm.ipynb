{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976ec90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle as pkl\n",
    "import array\n",
    "import os\n",
    "import timeit \n",
    "import contextlib\n",
    "import numpy as np\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87769d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: William Butler Yeats\n",
      "Wine comes in at the mouth\n",
      "And love comes in at the eye;\n",
      "That's all we shall know for truth\n",
      "Before we grow old and die.\n",
      "I lift the glass to my mouth,\n",
      "I look at you, and I sigh.\n"
     ]
    }
   ],
   "source": [
    "with open('dataset/A Drinking Song.txt', 'r') as f:\n",
    "    print(f.read())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0ecde73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义IDMAP，实现id和str的转换\n",
    "class IdMap:\n",
    "    \"\"\"Helper class to store a mapping from strings to ids.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.str_to_id = {}\n",
    "        self.id_to_str = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of terms stored in the IdMap\"\"\"\n",
    "        return len(self.id_to_str)\n",
    "        \n",
    "    def _get_str(self, i):\n",
    "        \"\"\"Returns the string corresponding to a given id (`i`).\"\"\"\n",
    "        ### Begin your code\n",
    "        return self.id_to_str[i]\n",
    "        ### End your code\n",
    "        \n",
    "    def _get_id(self, s):\n",
    "        \"\"\"Returns the id corresponding to a string (`s`). \n",
    "        If `s` is not in the IdMap yet, then assigns a new id and returns the new id.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        if s not in self.str_to_id:\n",
    "            self.id_to_str.append(s)\n",
    "            temp=self.id_to_str.index(s)\n",
    "            self.str_to_id[s]=temp\n",
    "            #返回新分配的id\n",
    "            return temp\n",
    "        \n",
    "        #如果s在，返回已存在的id\n",
    "        return self.str_to_id[s]\n",
    "        ### End your code\n",
    "            \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"If `key` is a integer, use _get_str; \n",
    "           If `key` is a string, use _get_id;\"\"\"\n",
    "        if type(key) is int:\n",
    "            return self._get_str(key)\n",
    "        elif type(key) is str:\n",
    "            return self._get_id(key)\n",
    "        else:\n",
    "            raise TypeError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a121ba",
   "metadata": {},
   "source": [
    "## 一、数据集处理、建立向量空间\n",
    "\n",
    "### （1）词袋生成、map构建\n",
    "如下为遍历数据集，生成标题、作者和内容的词袋。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ca6755",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_words_title:  ['a', 'aedh', 'are', 'by', 'can', 'cloths', 'down', 'drinking', 'fear', 'for', 'freedom', 'gardens', 'heaven', 'i', 'in', 'is', 'leave', 'me', 'mind', 'moonlight', 'of', 'old', 'salley', 'song', 'the', 'this', 'tonight', 'walk', 'when', 'where', 'wishes', 'with', 'without', 'write', 'you']\n",
      "bag_words_author: ['butler', 'knight', 'leon', 'neruda', 'pablo', 'rabindranath', 'tagore', 'william', 'yeats']\n",
      "bag_words_content:  ['a', 'adventurous', 'again', 'ages', 'agree', 'all', 'am', 'anarchy', 'and', 'are', 'arms', 'as', 'at', 'back', 'beads', 'beauty', 'because', 'beckoning', 'been', 'before', 'being', 'bending', 'bid', 'blind', 'blinding', 'blue', 'book', 'breaking', 'broken', 'burden', 'but', 'by', 'call', 'can', 'changing', 'chanting', 'charms', 'claim', 'clear', 'close', 'closer', 'cloths', 'cold', 'come', 'comes', 'corner', 'cup', 'dark', 'day', 'dead', 'death', 'deep', 'depth', 'desert', 'destiny', 'did', 'die', 'dim', 'distance', 'domestic', 'doors', 'dost', 'down', 'draw', 'dream', 'dreams', 'dreary', 'drink', 'easy', 'embroidered', 'endless', 'enwrought', 'ever', 'example', 'eye', 'eyes', 'face', 'false', 'fasten', 'fear', 'feel', 'feet', 'field', 'fire', 'foolish', 'for', 'fragments', 'free', 'freedom', 'from', 'full', 'future', 'gardens', 'glad', 'glass', 'god', 'golden', 'grace', 'grass', 'grey', 'ground', 'grow', 'grows', 'habit', 'had', 'half', 'hand', 'hard', 'has', 'have', 'he', 'head', 'heavens', 'held', 'helm', 'her', 'high', 'how', 'i', 'in', 'into', 'is', 'its', 'kissed', 'know', 'knowledge', 'laid', 'leaning', 'leave', 'leaves', 'life', 'lift', 'light', 'like', 'lines', 'little', 'lonely', 'look', 'lost', 'love', 'loved', 'magic', 'man', 'many', 'me', 'meet', 'mind', 'mistrusting', 'moments', 'moonlight', 'motherland', 'mouth', 'my', 'narrow', 'night', 'nights', 'nodding', 'not', 'now', 'of', 'old', 'on', 'once', 'one', 'only', 'open', 'or', 'our', 'out', 'passed', 'pathmaker', 'paths', 'perfection', 'pilgrim', 'poor', 'read', 'reason', 'rejoice', 'revolves', 'rigid', 'river', 'saddest', 'sails', 'salley', 'sand', 'see', 'shackles', 'shadows', 'shall', 'she', 'shiver', 'shoulder', 'shut', 'sigh', 'silver', 'singing', 'sings', 'sky', 'sleep', 'slowly', 'slumber', 'snow-white', 'soft', 'softly', 'sometimes', 'sorrows', 'soul', 'sparkles', 'speaks', 'spread', 'stand', 'star', 'starry', 'stars', 'stillness', 'stones', 'stream', 'stretches', 'striving', 'take', 'talk', 'tears', 'telling', 'temple', 'that', 'the', 'thee', 'their', 'there', 'thine', 'this', 'thou', 'through', 'thy', 'tiller', 'tilling', 'tireless', 'to', 'tonight', 'too', 'towards', 'tread', 'tree', 'true', 'truth', 'uncertain', 'under', 'up', 'walk', 'walls', 'warmth', 'was', 'way', 'we', 'weakly', 'weirs', 'when', 'where', 'wherewith', 'whole', 'whom', 'wind', 'winds', 'wine', 'with', 'within', 'without', 'words', 'world', 'worship', 'would', 'write', 'yielded', 'you', 'young', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "#分别为标题、作者和内容的词袋\n",
    "bag_words_title=[]\n",
    "bag_words_author=[]\n",
    "bag_words_content=[]\n",
    "\n",
    "#数据集地址\n",
    "dir=\"dataset/\"\n",
    "#文档的idmap\n",
    "doc_id_map=IdMap()\n",
    "#统计文档数\n",
    "num_doc=0\n",
    "\n",
    "for doc in os.listdir(dir):\n",
    "    num_doc+=1\n",
    "    #去除.txt\n",
    "    pre_doc=doc.strip()[0:-4].lower()\n",
    "    for term_title in pre_doc.split(' '):\n",
    "        #题目构成的词袋\n",
    "        bag_words_title.append(term_title)\n",
    "        \n",
    "    #添加doc和id的map\n",
    "    doc_id=doc_id_map[doc.strip()[0:-4]]\n",
    "\n",
    "    firstline=1\n",
    "    for line in open(dir+doc).readlines(): \n",
    "        if firstline==1:\n",
    "            line=line.strip()[8:].lower().split(' ')       \n",
    "            bag_words_author.extend(line)\n",
    "            firstline=0\n",
    "        else:\n",
    "            #去除停用词\n",
    "            line=line.replace(';','')\n",
    "            line=line.replace(\",\", '')\n",
    "            line=line.replace('.','')\n",
    "            line=line.replace(':','')\n",
    "            line=line.replace('?','')\n",
    "            line=line.replace('!','')\n",
    "            line=line.replace(\"'s\",'')\n",
    "            line=line.replace(\"'ll\",'')\n",
    "            line=line.replace(\"'\",'')  \n",
    "            line=line.strip().lower().split(' ')  \n",
    "            bag_words_content.extend(line)\n",
    "\n",
    "            \n",
    "            \n",
    "bag_words_title=sorted(set(bag_words_title))\n",
    "bag_words_author=sorted(set(bag_words_author))\n",
    "bag_words_content=sorted(set(bag_words_content))\n",
    "print(\"bag_words_title: \",bag_words_title)\n",
    "print(\"bag_words_author:\",bag_words_author)\n",
    "print(\"bag_words_content: \",bag_words_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09232437",
   "metadata": {},
   "source": [
    "根据生成标题、作者和内容的词袋构建IDMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb893679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "9\n",
      "277\n",
      "{'a': 0, 'aedh': 1, 'are': 2, 'by': 3, 'can': 4, 'cloths': 5, 'down': 6, 'drinking': 7, 'fear': 8, 'for': 9, 'freedom': 10, 'gardens': 11, 'heaven': 12, 'i': 13, 'in': 14, 'is': 15, 'leave': 16, 'me': 17, 'mind': 18, 'moonlight': 19, 'of': 20, 'old': 21, 'salley': 22, 'song': 23, 'the': 24, 'this': 25, 'tonight': 26, 'walk': 27, 'when': 28, 'where': 29, 'wishes': 30, 'with': 31, 'without': 32, 'write': 33, 'you': 34}\n",
      "{'butler': 0, 'knight': 1, 'leon': 2, 'neruda': 3, 'pablo': 4, 'rabindranath': 5, 'tagore': 6, 'william': 7, 'yeats': 8}\n",
      "{'a': 0, 'adventurous': 1, 'again': 2, 'ages': 3, 'agree': 4, 'all': 5, 'am': 6, 'anarchy': 7, 'and': 8, 'are': 9, 'arms': 10, 'as': 11, 'at': 12, 'back': 13, 'beads': 14, 'beauty': 15, 'because': 16, 'beckoning': 17, 'been': 18, 'before': 19, 'being': 20, 'bending': 21, 'bid': 22, 'blind': 23, 'blinding': 24, 'blue': 25, 'book': 26, 'breaking': 27, 'broken': 28, 'burden': 29, 'but': 30, 'by': 31, 'call': 32, 'can': 33, 'changing': 34, 'chanting': 35, 'charms': 36, 'claim': 37, 'clear': 38, 'close': 39, 'closer': 40, 'cloths': 41, 'cold': 42, 'come': 43, 'comes': 44, 'corner': 45, 'cup': 46, 'dark': 47, 'day': 48, 'dead': 49, 'death': 50, 'deep': 51, 'depth': 52, 'desert': 53, 'destiny': 54, 'did': 55, 'die': 56, 'dim': 57, 'distance': 58, 'domestic': 59, 'doors': 60, 'dost': 61, 'down': 62, 'draw': 63, 'dream': 64, 'dreams': 65, 'dreary': 66, 'drink': 67, 'easy': 68, 'embroidered': 69, 'endless': 70, 'enwrought': 71, 'ever': 72, 'example': 73, 'eye': 74, 'eyes': 75, 'face': 76, 'false': 77, 'fasten': 78, 'fear': 79, 'feel': 80, 'feet': 81, 'field': 82, 'fire': 83, 'foolish': 84, 'for': 85, 'fragments': 86, 'free': 87, 'freedom': 88, 'from': 89, 'full': 90, 'future': 91, 'gardens': 92, 'glad': 93, 'glass': 94, 'god': 95, 'golden': 96, 'grace': 97, 'grass': 98, 'grey': 99, 'ground': 100, 'grow': 101, 'grows': 102, 'habit': 103, 'had': 104, 'half': 105, 'hand': 106, 'hard': 107, 'has': 108, 'have': 109, 'he': 110, 'head': 111, 'heavens': 112, 'held': 113, 'helm': 114, 'her': 115, 'high': 116, 'how': 117, 'i': 118, 'in': 119, 'into': 120, 'is': 121, 'its': 122, 'kissed': 123, 'know': 124, 'knowledge': 125, 'laid': 126, 'leaning': 127, 'leave': 128, 'leaves': 129, 'life': 130, 'lift': 131, 'light': 132, 'like': 133, 'lines': 134, 'little': 135, 'lonely': 136, 'look': 137, 'lost': 138, 'love': 139, 'loved': 140, 'magic': 141, 'man': 142, 'many': 143, 'me': 144, 'meet': 145, 'mind': 146, 'mistrusting': 147, 'moments': 148, 'moonlight': 149, 'motherland': 150, 'mouth': 151, 'my': 152, 'narrow': 153, 'night': 154, 'nights': 155, 'nodding': 156, 'not': 157, 'now': 158, 'of': 159, 'old': 160, 'on': 161, 'once': 162, 'one': 163, 'only': 164, 'open': 165, 'or': 166, 'our': 167, 'out': 168, 'passed': 169, 'pathmaker': 170, 'paths': 171, 'perfection': 172, 'pilgrim': 173, 'poor': 174, 'read': 175, 'reason': 176, 'rejoice': 177, 'revolves': 178, 'rigid': 179, 'river': 180, 'saddest': 181, 'sails': 182, 'salley': 183, 'sand': 184, 'see': 185, 'shackles': 186, 'shadows': 187, 'shall': 188, 'she': 189, 'shiver': 190, 'shoulder': 191, 'shut': 192, 'sigh': 193, 'silver': 194, 'singing': 195, 'sings': 196, 'sky': 197, 'sleep': 198, 'slowly': 199, 'slumber': 200, 'snow-white': 201, 'soft': 202, 'softly': 203, 'sometimes': 204, 'sorrows': 205, 'soul': 206, 'sparkles': 207, 'speaks': 208, 'spread': 209, 'stand': 210, 'star': 211, 'starry': 212, 'stars': 213, 'stillness': 214, 'stones': 215, 'stream': 216, 'stretches': 217, 'striving': 218, 'take': 219, 'talk': 220, 'tears': 221, 'telling': 222, 'temple': 223, 'that': 224, 'the': 225, 'thee': 226, 'their': 227, 'there': 228, 'thine': 229, 'this': 230, 'thou': 231, 'through': 232, 'thy': 233, 'tiller': 234, 'tilling': 235, 'tireless': 236, 'to': 237, 'tonight': 238, 'too': 239, 'towards': 240, 'tread': 241, 'tree': 242, 'true': 243, 'truth': 244, 'uncertain': 245, 'under': 246, 'up': 247, 'walk': 248, 'walls': 249, 'warmth': 250, 'was': 251, 'way': 252, 'we': 253, 'weakly': 254, 'weirs': 255, 'when': 256, 'where': 257, 'wherewith': 258, 'whole': 259, 'whom': 260, 'wind': 261, 'winds': 262, 'wine': 263, 'with': 264, 'within': 265, 'without': 266, 'words': 267, 'world': 268, 'worship': 269, 'would': 270, 'write': 271, 'yielded': 272, 'you': 273, 'young': 274, 'your': 275, 'yourself': 276}\n"
     ]
    }
   ],
   "source": [
    "#分别构建map\n",
    "print(len(bag_words_title))\n",
    "print(len(bag_words_author))\n",
    "print(len(bag_words_content))\n",
    "\n",
    "title_term_id_map=IdMap()\n",
    "author_term_id_map=IdMap()\n",
    "content_term_id_map=IdMap()\n",
    "\n",
    "for temp in bag_words_title:\n",
    "    t=title_term_id_map[temp]\n",
    "    \n",
    "for temp in bag_words_author:\n",
    "    t=author_term_id_map[temp]\n",
    "    \n",
    "for temp in bag_words_content:\n",
    "    t=content_term_id_map[temp]\n",
    "\n",
    "print(title_term_id_map.str_to_id)\n",
    "print(author_term_id_map.str_to_id)\n",
    "print(content_term_id_map.str_to_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c19b87",
   "metadata": {},
   "source": [
    "### （2）生成tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39ce8094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n之后的计算中TF = log10( N+1)，减少文本长度带来的应影响\\n\\n向量形如：\\nterm  doc1    doc2\\na     0       2\\nb     1       2\\nc     0       1\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf向量\n",
    "tf_title=np.zeros((len(bag_words_title),num_doc))\n",
    "tf_author=np.zeros((len(bag_words_author),num_doc))\n",
    "tf_content=np.zeros((len(bag_words_content),num_doc))\n",
    "print(tf_author.shape)\n",
    "\n",
    "\n",
    "'''\n",
    "之后的计算中TF = log10( N+1)，减少文本长度带来的应影响\n",
    "\n",
    "向量形如：\n",
    "term  doc1    doc2\n",
    "a     0       2\n",
    "b     1       2\n",
    "c     0       1\n",
    "'''\n",
    "\n",
    "#统计文档数\n",
    "doc_count=0\n",
    "\n",
    "#遍历数据集\n",
    "for doc in os.listdir(dir):\n",
    "    #将得到的标题进行预处理\n",
    "    pre_doc=doc.strip()[0:-4].lower()\n",
    "    #根据预处理后的标题构建tf向量\n",
    "    for term_title in pre_doc.split(' '):\n",
    "        term_id=title_term_id_map._get_id(term_title)\n",
    "        tf_title[term_id][doc_count]+=1\n",
    "    \n",
    "\n",
    "    firstline=1\n",
    "    for line in open(dir+doc).readlines(): \n",
    "        #判断是否为第一行，第一行为作者\n",
    "        if firstline==1:\n",
    "            #作者信息预处理\n",
    "            line=line.strip()[8:].lower().split(' ')\n",
    "            #构建作者的tf向量\n",
    "            for term_author in line:\n",
    "                author_id=author_term_id_map._get_id(term_author)\n",
    "                tf_author[author_id][doc_count]+=1\n",
    "            firstline=0\n",
    "        #内容   \n",
    "        else:\n",
    "            #去除停用词\n",
    "            line=line.replace(';','')\n",
    "            line=line.replace(\",\", '')\n",
    "            line=line.replace('.','')\n",
    "            line=line.replace(':','')\n",
    "            line=line.replace('?','')\n",
    "            line=line.replace('!','')\n",
    "            line=line.replace(\"'s\",'')\n",
    "            line=line.replace(\"'ll\",'')\n",
    "            line=line.replace(\"'\",'')\n",
    "            #内容预处理\n",
    "            line=line.strip().lower().split(' ')\n",
    "            #构建内容的tf向量\n",
    "            for term_content in line:\n",
    "                content_id=content_term_id_map._get_id(term_content)\n",
    "                tf_content[content_id][doc_count]+=1\n",
    "            \n",
    "    doc_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e91478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'butler': 0, 'knight': 1, 'leon': 2, 'neruda': 3, 'pablo': 4, 'rabindranath': 5, 'tagore': 6, 'william': 7, 'yeats': 8}\n",
      "[[1. 1. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(author_term_id_map.str_to_id)\n",
    "print(tf_author)\n",
    "#print(title_term_id_map.str_to_id)\n",
    "#print(tf_title)\n",
    "#print(content_term_id_map.str_to_id)\n",
    "#print(tf_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6600509c",
   "metadata": {},
   "source": [
    "### （3）生成df、idf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82f5895f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n其中IDF = log10( N/df)，其中N为文档数,df为含有对应term的文档数\\n\\n向量形如：\\nterm  df  idf\\na    2  log10(N/2)\\nb    1  log10(N/1)\\nc    1  log10(N/1)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.         0.35218252]\n",
      " [1.         0.95424251]\n",
      " [1.         0.95424251]\n",
      " [1.         0.95424251]\n",
      " [1.         0.95424251]\n",
      " [3.         0.47712125]\n",
      " [3.         0.47712125]\n",
      " [4.         0.35218252]\n",
      " [4.         0.35218252]]\n"
     ]
    }
   ],
   "source": [
    "#df和idf向量\n",
    "\n",
    "df_title=np.zeros((len(bag_words_title),2))\n",
    "df_author=np.zeros((len(bag_words_author),2))\n",
    "df_content=np.zeros((len(bag_words_content),2))\n",
    "\n",
    "'''\n",
    "其中IDF = log10( N/df)，其中N为文档数,df为含有对应term的文档数\n",
    "\n",
    "向量形如：\n",
    "term  df  idf\n",
    "a    2  log10(N/2)\n",
    "b    1  log10(N/1)\n",
    "c    1  log10(N/1)\n",
    "'''\n",
    "\n",
    "#生成term对应的df和idf\n",
    "def df(len,num_doc,tf_x,df_x):\n",
    "    for i in range(len):\n",
    "        count=0\n",
    "        for j in range(num_doc):\n",
    "            if tf_x[i][j]!=0:\n",
    "                count+=1\n",
    "        df_x[i][0]=count\n",
    "        df_x[i][1]=math.log10(num_doc/count)\n",
    "\n",
    "\n",
    "df(len(bag_words_title),num_doc,tf_title,df_title)\n",
    "df(len(bag_words_author),num_doc,tf_author,df_author)\n",
    "df(len(bag_words_content),num_doc,tf_content,df_content)\n",
    "    \n",
    "\n",
    "\n",
    "print(df_author)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee989fd",
   "metadata": {},
   "source": [
    "### （4）tf-idf\n",
    "分别生成文档的标题、作者、内容的tf-idf向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fda8ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1060175  0.         0.         0.         0.         0.\n",
      "  0.         0.1060175  0.1060175 ]\n",
      " [0.1060175  0.         0.         0.         0.         0.\n",
      "  0.         0.1060175  0.1060175 ]\n",
      " [0.1060175  0.         0.         0.         0.         0.\n",
      "  0.         0.1060175  0.1060175 ]\n",
      " [0.         0.         0.         0.         0.         0.14362781\n",
      "  0.14362781 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.14362781\n",
      "  0.14362781 0.         0.        ]\n",
      " [0.         0.         0.         0.28725562 0.28725562 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.28725562 0.28725562 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.1060175  0.         0.         0.         0.         0.\n",
      "  0.         0.1060175  0.1060175 ]\n",
      " [0.         0.         0.         0.         0.         0.14362781\n",
      "  0.14362781 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#tf-idf\n",
    "\n",
    "\n",
    "tf_idf_title=np.zeros((num_doc,len(bag_words_title)))\n",
    "tf_idf_author=np.zeros((num_doc,len(bag_words_author)))\n",
    "tf_idf_content=np.zeros((num_doc,len(bag_words_content)))\n",
    "\n",
    "#生成文档对对应的tf-idf向量\n",
    "def tf_idf(num_doc,len,tf_idf_x,tf_x,df_x):\n",
    "    \n",
    "    for i in range(num_doc):\n",
    "        for j in range(len):\n",
    "            tf_idf_x[i][j]=math.log10(tf_x[j][i]+1)*df_x[j][1]\n",
    "            \n",
    "            \n",
    "tf_idf(num_doc,len(bag_words_title),tf_idf_title,tf_title,df_title)\n",
    "tf_idf(num_doc,len(bag_words_author),tf_idf_author,tf_author,df_author)\n",
    "tf_idf(num_doc,len(bag_words_content),tf_idf_content,tf_content,df_content)\n",
    "print(tf_idf_author)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e10811",
   "metadata": {},
   "source": [
    "### （5）序列化储存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a365f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"pkl_dir\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "with open('pkl_dir/doc.pkl','wb') as doc:\n",
    "    pkl.dump((num_doc,doc_id_map),doc)\n",
    "\n",
    "\n",
    "with open('pkl_dir/term_map.pkl','wb') as tp:\n",
    "    pkl.dump((title_term_id_map,author_term_id_map,content_term_id_map),tp)\n",
    "\n",
    "with open('pkl_dir/bag_words.pkl','wb') as bw:\n",
    "    pkl.dump((bag_words_title,bag_words_author,bag_words_content),bw)\n",
    "\n",
    "with open('pkl_dir/tf.pkl','wb') as tf:\n",
    "    pkl.dump((tf_title,tf_author,tf_content),tf)\n",
    "    \n",
    "with open('pkl_dir/df.pkl','wb') as df:\n",
    "    pkl.dump((df_title,df_author,df_content),df)\n",
    "    \n",
    "with open('pkl_dir/tf_idf.pkl','wb') as tf_idf :\n",
    "    pkl.dump((tf_idf_title,tf_idf_author,tf_idf_content),tf_idf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8c33c",
   "metadata": {},
   "source": [
    "以上将数据集处理完毕并存储"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ec3a3",
   "metadata": {},
   "source": [
    "## 二、查询\n",
    "### （1）读取存储的向量及相关信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13654cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import array\n",
    "import os\n",
    "import timeit \n",
    "import contextlib\n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "#定义IDMAP，实现id和str的转换\n",
    "class IdMap:\n",
    "    \"\"\"Helper class to store a mapping from strings to ids.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.str_to_id = {}\n",
    "        self.id_to_str = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of terms stored in the IdMap\"\"\"\n",
    "        return len(self.id_to_str)\n",
    "        \n",
    "    def _get_str(self, i):\n",
    "        \"\"\"Returns the string corresponding to a given id (`i`).\"\"\"\n",
    "        ### Begin your code\n",
    "        return self.id_to_str[i]\n",
    "        ### End your code\n",
    "        \n",
    "    def _get_id(self, s):\n",
    "        \"\"\"Returns the id corresponding to a string (`s`). \n",
    "        If `s` is not in the IdMap yet, then assigns a new id and returns the new id.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        if s not in self.str_to_id:\n",
    "            self.id_to_str.append(s)\n",
    "            temp=self.id_to_str.index(s)\n",
    "            self.str_to_id[s]=temp\n",
    "            #返回新分配的id\n",
    "            return temp\n",
    "        \n",
    "        #如果s在，返回已存在的id\n",
    "        return self.str_to_id[s]\n",
    "        ### End your code\n",
    "            \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"If `key` is a integer, use _get_str; \n",
    "           If `key` is a string, use _get_id;\"\"\"\n",
    "        if type(key) is int:\n",
    "            return self._get_str(key)\n",
    "        elif type(key) is str:\n",
    "            return self._get_id(key)\n",
    "        else:\n",
    "            raise TypeError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5377f3",
   "metadata": {},
   "source": [
    "反序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b6820e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bag_words_title=[]\n",
    "bag_words_author=[]\n",
    "bag_words_content=[]\n",
    "num_doc=0\n",
    "doc_id_map=IdMap()\n",
    "\n",
    "with open('pkl_dir/doc.pkl','rb') as doc:\n",
    "    num_doc,doc_id_map= pkl.load(doc)\n",
    "\n",
    "with open('pkl_dir/bag_words.pkl','rb') as bw:\n",
    "    bag_words_title,bag_words_author,bag_words_content= pkl.load(bw)\n",
    "\n",
    "tf_title=np.zeros((len(bag_words_title),num_doc))\n",
    "tf_author=np.zeros((len(bag_words_author),num_doc))\n",
    "tf_content=np.zeros((len(bag_words_content),num_doc))\n",
    "    \n",
    "title_term_id_map=IdMap()\n",
    "author_term_id_map=IdMap()\n",
    "content_term_id_map=IdMap()  \n",
    "\n",
    "with open('pkl_dir/term_map.pkl','rb') as tp:\n",
    "    title_term_id_map,author_term_id_map,content_term_id_map=pkl.load(tp)\n",
    "\n",
    "with open('pkl_dir/tf.pkl','rb') as tf:\n",
    "    tf_title,tf_author,tf_content=pkl.load(tf)\n",
    "    \n",
    "df_title=np.zeros((len(bag_words_title),2))\n",
    "df_author=np.zeros((len(bag_words_author),2))\n",
    "df_content=np.zeros((len(bag_words_content),2))\n",
    "    \n",
    "with open('pkl_dir/df.pkl','rb') as df:\n",
    "    df_title,df_author,df_content=pkl.load(df)\n",
    "    \n",
    "tf_idf_title=np.zeros((num_doc,len(bag_words_title)))\n",
    "tf_idf_author=np.zeros((num_doc,len(bag_words_author)))\n",
    "tf_idf_content=np.zeros((num_doc,len(bag_words_content)))\n",
    "    \n",
    "with open('pkl_dir/tf_idf.pkl','rb') as tf_idf :\n",
    "    tf_idf_title,tf_idf_author,tf_idf_content=pkl.load(tf_idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ef463",
   "metadata": {},
   "source": [
    "### (2) 输入的处理、生成检索向量的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a52949ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建输入检索的向量\n",
    "\n",
    "#输入的预处理\n",
    "def pre_input(line):\n",
    "         #去除停用词\n",
    "        line=line.replace(';','')\n",
    "        line=line.replace(\",\", '')\n",
    "        line=line.replace('.','')\n",
    "        line=line.replace(':','')\n",
    "        line=line.replace('?','')\n",
    "        line=line.replace('!','')\n",
    "        line=line.replace(\"'s\",'')\n",
    "        line=line.replace(\"'ll\",'')\n",
    "        line=line.replace(\"'\",'')\n",
    "        #内容预处理\n",
    "        line=line.strip().lower().split(' ')\n",
    "        \n",
    "        return line\n",
    "\n",
    "def query_vector(input_query,len,bag_words_x,x_term_id_map,df_x,x_query_vector):\n",
    "    bag_words=pre_input(input_query)\n",
    "    \n",
    "    tf=np.zeros(len)\n",
    "    for term in bag_words:\n",
    "        if term in bag_words_x:\n",
    "            term_id=x_term_id_map._get_id(term)\n",
    "            tf[term_id]+=1\n",
    "\n",
    "    \n",
    "    for i in range(len):\n",
    "        x_query_vector[i]=math.log10(tf[i]+1)*df_x[i][1]\n",
    "    \n",
    "    return tf_idf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37818ced",
   "metadata": {},
   "source": [
    "### (3) 查询输入、根据查询生成查询向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd7737a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please input the title you want to search: \n",
      "please input the author you want to search: \n",
      "please input the content you want to search: I would spread the cloths under your feet:Drink deep the cup of moonlight; Drink deep the magic charms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_io.BufferedReader name='pkl_dir/tf_idf.pkl'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#输入\n",
    "title_query=input(\"please input the title you want to search: \")\n",
    "author_query=input(\"please input the author you want to search: \")\n",
    "content_query=input(\"please input the content you want to search: \")\n",
    "\n",
    "#生成输入的检索的向量\n",
    "title_query_vector=np.zeros(len(bag_words_title))\n",
    "author_query_vector=np.zeros(len(bag_words_author))\n",
    "content_query_vector=np.zeros(len(bag_words_content))\n",
    "                            \n",
    "#独热码，有相应查询，设为1，生成对应查询向量                           \n",
    "query=[0,0,0]\n",
    "if title_query!='':\n",
    "    query[0]=1\n",
    "    query_vector(title_query,len(bag_words_title),bag_words_title,title_term_id_map,df_title,title_query_vector)\n",
    "if author_query!='':\n",
    "    query[1]=1\n",
    "    query_vector(author_query,len(bag_words_author),bag_words_author,author_term_id_map,df_author,author_query_vector)\n",
    "if content_query!='':\n",
    "    query[2]=1\n",
    "    query_vector(content_query,len(bag_words_content),bag_words_content,content_term_id_map,df_content,content_query_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df2c153",
   "metadata": {},
   "source": [
    "### （4）计算得分，排序，输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1784a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk with Me in Moonlight\n",
      "Aedh wishes for the Cloths of Heaven\n",
      "When you are old\n",
      "Tonight I Can Write\n",
      "Down by the Salley Gardens\n",
      "Freedom\n",
      "A Drinking Song\n",
      "Where the Mind is Without Fear\n",
      "Leave This\n"
     ]
    }
   ],
   "source": [
    "#各个文档分数\n",
    "score_title=np.zeros(num_doc)\n",
    "score_author=np.zeros(num_doc)\n",
    "score_content=np.zeros(num_doc)\n",
    "\n",
    "#分配权重\n",
    "w_title=5\n",
    "w_author=3\n",
    "w_content=2\n",
    "w_sum=0\n",
    "\n",
    "\n",
    "#cos\n",
    "def cosine_similarity(x,y):\n",
    "    num = x.dot(y.T)\n",
    "    denom = np.linalg.norm(x) * np.linalg.norm(y)\n",
    "    return num / denom\n",
    "\n",
    "#分数计算\n",
    "def get_score(len,score,tf_idf_x,query):\n",
    "    \n",
    "    for i in range(len):\n",
    "        score[i]=cosine_similarity(tf_idf_x[i],query)\n",
    "\n",
    "#根据独热码，计算单个域的得分向量，计算对应的权重和\n",
    "if query[0]==1:\n",
    "    get_score(num_doc,score_title,tf_idf_title,title_query_vector)\n",
    "    w_sum+=w_title\n",
    "if query[1]==1:\n",
    "    get_score(num_doc,score_author,tf_idf_author,author_query_vector)\n",
    "    w_sum+=w_author\n",
    "if query[2]==1:\n",
    "    get_score(num_doc,score_content,tf_idf_content,content_query_vector)\n",
    "    w_sum+=w_content\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "#文档分数向量\n",
    "\n",
    "score=np.zeros(num_doc)\n",
    "for i in range(num_doc):\n",
    "    if query[0]==1:\n",
    "         score[i]+=(w_title/w_sum)*score_title[i]\n",
    "    if query[1]==1:\n",
    "        score[i]+=(w_author/w_sum)*score_author[i]\n",
    "    if query[2]==1:\n",
    "        score[i]+=(w_content/w_sum)*score_content[i]\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#根据相关性得分排序，去除得分为0文档，其他文档按从大到小排序\n",
    "sorted_score = sorted(enumerate(score), key=lambda score:score[1],reverse=True)  \n",
    "sorted_score_id = [score[0] for score in sorted_score if score[1]!=0] \n",
    "\n",
    "\n",
    "#根据排名输出\n",
    "for i in sorted_score_id:\n",
    "    str=doc_id_map._get_str(i)\n",
    "    print(str)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe58ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9af9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
